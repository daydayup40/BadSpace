{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始数据20000条\n"
     ]
    }
   ],
   "source": [
    "#按行读取数据\n",
    "file = open('/ml/sougou/user_tag_query.2W.TRAIN')\n",
    "file_lines = file.readlines()\n",
    "file_linenum = len(file_lines)\n",
    "file.close()\n",
    "\n",
    "#age:    0：未知年龄; 1：0-18岁; 2：19-23岁; 3：24-30岁; 4：31-40岁; 5：41-50岁; 6： 51-999岁\n",
    "#gender: 0：未知1：男性2：女性\n",
    "#edu:    0：未知学历; 1：博士; 2：硕士; 3：大学生; 4：高中; 5：初中; 6：小学\n",
    "\n",
    "#通过正则表达式说去数据\n",
    "import re\n",
    "pattern = '([0-9A-Z]*)\\s([0-6]{1})\\s([0-2]{1})\\s([0-6]{1})\\s(.*)'\n",
    "\n",
    "#strIDs = [1] * file_linenum\n",
    "IDlist = []\n",
    "Agelist = []\n",
    "Genderlist = []\n",
    "Edulist = []\n",
    "Datalist = []\n",
    "\n",
    "nRealSize = 0\n",
    "\n",
    "for i in range( 0 , file_linenum ):\n",
    "    m = re.search( pattern , file_lines[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        nRealSize += 1\n",
    "        IDlist.append(m.group(1))\n",
    "        Agelist.append(m.group(2))\n",
    "        Genderlist.append(m.group(3))\n",
    "        Edulist.append(m.group(4))\n",
    "        Datalist.append(m.group(5))\n",
    "    \n",
    "print( \"读取原始数据%d条\" % (nRealSize) )\n",
    "\n",
    "#wwws = mycut( Datalist[1000] )\n",
    "#print ','.join(wwws)\n",
    "#print (len(wwws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取停用词2612条\n",
      "读取切分后数据20000条\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "stopwords = [line.strip().decode('utf-8', 'ignore') for line in open('/ml/sougou/stopwords.txt').readlines()]\n",
    "print( \"读取停用词%d条\" % (len(stopwords)) )\n",
    "\n",
    "def mycut( strCnText ):\n",
    "    words = pseg.cut(strCnText)\n",
    "    nword = []\n",
    "    for w in words:\n",
    "        #print( w.word + \" \" + w.flag )\n",
    "        #去除一些不关心的词:[c  连词][p  介词][q  量词][t  时间词][m  量词][eng  英文字母]\n",
    "        if((w.flag != 'c' and w.flag != 'p' and w.flag != 'q' and w.flag != 't'and w.flag != 'm'and w.flag != 'eng') and len(w.word)>1):\n",
    "            #通过停用词库去除一些不需要考虑的词汇\n",
    "            if w.word not in stopwords:\n",
    "                nword.append(w.word)\n",
    "            #else:\n",
    "                #print( \"STOP:\" + w.word + \" \" + w.flag )\n",
    "        #else:\n",
    "            #print( \"CX  :\" + w.word + \" \" + w.flag )\n",
    "            \n",
    "    return nword\n",
    "\n",
    "#开始对数据进行读取\n",
    "org_cut_data_file = '/ml/sougou/orgcut.dat'\n",
    "def create_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == True ):\n",
    "        cget = raw_input(\"File exist,cover it?(y / n)\")\n",
    "        if ( cget == 'y' ):\n",
    "            os.remove( org_cut_data_file )\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    #创建文件\n",
    "    print( \"开始创建文件.....\" )\n",
    "    fp = open( org_cut_data_file , 'w' )\n",
    "    for i in range( 0 , nRealSize ):\n",
    "    #for i in range( 0 , 10 ):\n",
    "        if ( i % 100 == 0 ):\n",
    "            print( \"%d / %d\" % ( i , nRealSize ) )\n",
    "            fp.flush()\n",
    "        fp.write( ','.join(mycut(Datalist[i])).encode('utf') + '\\n' )\n",
    "    fp.close()\n",
    "        \n",
    "#加载数据\n",
    "def load_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == False ):\n",
    "        create_org_cut_data()\n",
    "    lines = [line.strip().decode('utf', 'ignore') for line in open(org_cut_data_file).readlines()]\n",
    "    orgcutdata = []\n",
    "    for line in lines:\n",
    "        orgcutdata.append( line.split(',') )\n",
    "    return orgcutdata\n",
    "\n",
    "#对数据进行二次处理，一方面是将数据改为空格间隔，另一方面是可以对数据进行进一步的处理\n",
    "def org_to_final_cut_data(org_cut_data):\n",
    "    cutdata = []\n",
    "    for words in org_cut_data:\n",
    "        #这里可以对words进行进一步的调整\n",
    "        \n",
    "        \n",
    "        cutdata.append( ' '.join(words) )\n",
    "        #cutdata.append( ' '.join(set(words)) )\n",
    "        \n",
    "    return cutdata\n",
    "        \n",
    "    \n",
    "#create_org_cut_data()\n",
    "final_cut_data = org_to_final_cut_data( load_org_cut_data() )\n",
    "print( \"读取切分后数据%d条\" % len(final_cut_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#部分数据清除？\n",
    "for i in range( 0 , nRealSize )[::-1]:\n",
    "    if Genderlist[i] == '0':\n",
    "        IDlist.pop(i)\n",
    "        Genderlist.pop(i)\n",
    "        final_cut_data.pop(i)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:13703 , test:5873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#切分数据\n",
    "\n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "  \n",
    "doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Genderlist, test_size = 0.3)  \n",
    "\n",
    "print( \"train:%d , test:%d\" % ( len(doc_terms_train) , len(doc_terms_test) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]]\n",
      "表格 求和 平均数 大哥 花生 杨洋 择偶 标准 微信 服务 吸粉 活动 策划 吸粉 投票 活动 策划 定制 益力 软糖 酸奶 敷脸 好处 南京 媒体 运营 招聘 南京 智联 招聘 试吃 名单 公布 星座 零食 乐蜂 聚美 优品 九门 现场 良品 铺子 内容 运营 杨洋 粉刺 吃货 图片 中奖 葡萄 寒性 水果 苹果 年轻 撑腰 点击 购买 有趣 星座 零食 玛璐托 香蕉 巧克力 蛋糕 芜湖 智联 招聘 技能 宏亚 月光 舞曲 礼盒 青云 南京 媒体 运营 双鱼座 短信 营销 开心 茶馆 南京 皮炎 主题 威化 图片 微信 公众 分享 补水 效果 护肤品 定制 养乐多 软糖 微信 公众 活动 策划 方案 吃货 晒图 活动 玛璐托 活动 策划 方案 主题 运营 招聘 捷森 麦片 微信 公众 吸粉 成功 案例 茶馆 开心 茶馆 南京 简笔画 摩羯座 白羊座 惩罚 男生 方法 大全 超狠 星座 零食 蜡笔小新 羊羹 月饼 点击 购买 图标 素材 夏黑 葡萄 提子 鼻子 旁边 红血丝 聊天 晚安 校园生活 威化 专员 岗位职责 黑头 小窍门 简单 龙门 飞爪 好吃 引导 购买 吃货 技巧 韩美 釜山 朋友圈 互动 话题 羊羹 葡萄 好吃 点击 购买 图标 肯德基 推出 防晒霜 卡萨 京都 原味 奶茶 智联 招聘 写给 会员 食代 零食 专题 送礼 误认为 有钱 体验 淘宝 系统 微信 公众 吸粉 活动 分享 南京 运营 招聘 零食 主题 南京 朗睿 科技 葡萄 引导 扫描 二维码 广告语 写给 会员 一段话 黑头 药物 治疗 免费 试吃 报名 方法 主题 嘉宝 引导 扫描 二维码 搞笑 广告语 好看 人字 微信 公众 服务 吸粉 活动 星七 粒粒 威化 摩羯座 美伦多 榴莲 益力 软糖 团圆 江苏 地图 吃货 会员 活动 策划 方案 微信 公众 活动 互动 话题 变得 传播 美食 定个 目标 客户 管理 品牌 点击 购买 试吃 统一 留言 巧克力 韩国 春雨 面膜 蜡笔小新 羊羹 表格 加减乘除 混合 运算 白色 恋人\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#转换\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer=TfidfTransformer()\n",
    "\n",
    "#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "x_train = transformer.fit_transform(vectorizer.fit_transform(doc_terms_train))\n",
    "x_test = transformer.transform(vectorizer.transform(doc_terms_test))\n",
    "\n",
    "#x_train = vectorizer.fit_transform(doc_terms_train)\n",
    "#x_test = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "print( vectorizer.fit_transform(doc_terms_train)[0].todense() )\n",
    "print( doc_terms_train[0] )\n",
    "print( x_train[0].todense() )\n",
    "print( y_train[0] )\n",
    "\n",
    "x2_train = []\n",
    "for n in x_train:\n",
    "    x2_train.append( n.todense() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "准确率:0.792780521028\n"
     ]
    }
   ],
   "source": [
    "#二分\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "nRight = 0\n",
    "for i in range( 0 , len(doc_terms_test) ):\n",
    "    if ( predictions[i] == y_test[i] ):\n",
    "        nRight += 1\n",
    "print( nRight * 100 / len(doc_terms_test) )\n",
    "print('准确率:%s' % accuracy_score(y_test, predictions))\n",
    "#scores = cross_val_score(classifier, x_train, y_train, cv=5)\n",
    "#print(' 准确率:',np.mean(scores), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00a3277e20ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;34m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#高斯朴素贝叶斯  \n",
    "import numpy as np \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "clf = GaussianNB().fit(x_train.todense(), y_train )\n",
    "predictions = clf.predict( x_test )\n",
    "\n",
    "nRight = 0\n",
    "for i in range( 0 , len(doc_terms_test) ):\n",
    "    if ( predictions[i] == y_test[i] ):\n",
    "        nRight += 1\n",
    "print( nRight * 100 / len(doc_terms_test) )\n",
    "print('准确率:%s' % accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
