{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始数据20000条\n",
      "读取停用词1893条\n",
      "读取切分后数据20000条\n",
      "原有数据个数:20000\n",
      "剔除未知之后数据个数:19576\n"
     ]
    }
   ],
   "source": [
    "#全局数据\n",
    "IDlist = []\n",
    "Agelist = []\n",
    "Genderlist = []\n",
    "Edulist = []\n",
    "Datalist = []\n",
    "\n",
    "\n",
    "#按行读取数据\n",
    "file = open('/ml/sougou/user_tag_query.2W.TRAIN')\n",
    "file_lines = file.readlines()\n",
    "file_linenum = len(file_lines)\n",
    "file.close()\n",
    "\n",
    "#age:    0：未知年龄; 1：0-18岁; 2：19-23岁; 3：24-30岁; 4：31-40岁; 5：41-50岁; 6： 51-999岁\n",
    "#gender: 0：未知1：男性2：女性\n",
    "#edu:    0：未知学历; 1：博士; 2：硕士; 3：大学生; 4：高中; 5：初中; 6：小学\n",
    "\n",
    "#通过正则表达式说去数据\n",
    "import re\n",
    "pattern = '([0-9A-Z]*)\\s([0-6]{1})\\s([0-2]{1})\\s([0-6]{1})\\s(.*)'\n",
    "\n",
    "IDlist = []\n",
    "Agelist = []\n",
    "Genderlist = []\n",
    "Edulist = []\n",
    "Datalist = []\n",
    "\n",
    "for i in range( 0 , file_linenum ):\n",
    "    m = re.search( pattern , file_lines[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        IDlist.append(m.group(1))\n",
    "        Agelist.append(m.group(2))\n",
    "        Genderlist.append(m.group(3))\n",
    "        Edulist.append(m.group(4))\n",
    "        Datalist.append(m.group(5))\n",
    "\n",
    "print( \"读取原始数据%d条\" % (len(IDlist)) )\n",
    "\n",
    "\n",
    "#文本处理\n",
    "import io\n",
    "import os\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "stopwords = [line.strip().decode('utf-8', 'ignore') for line in open('/ml/sougou/stopword1').readlines()]\n",
    "print( \"读取停用词%d条\" % (len(stopwords)) )\n",
    "\n",
    "def mycut( strCnText ):\n",
    "    words = pseg.cut(strCnText)\n",
    "    nword = []\n",
    "    for w in words:\n",
    "        #去除一些不关心的词:[c  连词][p  介词][q  量词][t  时间词][m  量词][eng  英文字母]\n",
    "        if((w.flag != 'c' and w.flag != 'p' and w.flag != 'q' and w.flag != 't'and w.flag != 'm'and w.flag != 'eng') and len(w.word)>1):\n",
    "            #通过停用词库去除一些不需要考虑的词汇\n",
    "            if w.word not in stopwords:\n",
    "                nword.append(w.word)\n",
    "            \n",
    "    return nword\n",
    "\n",
    "#开始对数据进行读取\n",
    "org_cut_data_file = '/ml/sougou/orgcut.dat'\n",
    "def create_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == True ):\n",
    "        cget = raw_input(\"File exist,cover it?(y / n)\")\n",
    "        if ( cget == 'y' ):\n",
    "            os.remove( org_cut_data_file )\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    #创建文件\n",
    "    print( \"开始创建文件.....\" )\n",
    "    fp = open( org_cut_data_file , 'w' )\n",
    "    for i in range( 0 , nRealSize ):\n",
    "    #for i in range( 0 , 10 ):\n",
    "        if ( i % 100 == 0 ):\n",
    "            print( \"%d / %d\" % ( i , nRealSize ) )\n",
    "            fp.flush()\n",
    "        fp.write( ','.join(mycut(Datalist[i])).encode('utf') + '\\n' )\n",
    "    fp.close()\n",
    "        \n",
    "#加载数据\n",
    "def load_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == False ):\n",
    "        create_org_cut_data()\n",
    "    lines = [line.strip().decode('utf', 'ignore') for line in open(org_cut_data_file).readlines()]\n",
    "    orgcutdata = []\n",
    "    for line in lines:\n",
    "        orgcutdata.append( line.split(',') )\n",
    "    return orgcutdata\n",
    "\n",
    "#对数据进行二次处理，一方面是将数据改为空格间隔，另一方面是可以对数据进行进一步的处理\n",
    "def org_to_final_cut_data(org_cut_data):\n",
    "    cutdata = []\n",
    "    for words in org_cut_data:\n",
    "        #这里可以对words进行进一步的调整\n",
    "        '''\n",
    "        wordslen = len( words )\n",
    "        for i in  range(0,wordslen)[::-1]:\n",
    "            if ( len( words[i] ) != 2 ):\n",
    "                words.pop(i)\n",
    "        '''     \n",
    "        cutdata.append( ' '.join(words) )\n",
    "        #cutdata.append( ' '.join(set(words)) )\n",
    "        \n",
    "    return cutdata\n",
    "\n",
    "org_cut_data = load_org_cut_data()\n",
    "final_cut_data = org_to_final_cut_data( org_cut_data )\n",
    "print( \"读取切分后数据%d条\" % len(final_cut_data) )\n",
    "\n",
    "#这里增加一个对性别位置数据的剔除工作\n",
    "print ( \"原有数据个数:%d\" % (len(org_cut_data)) )\n",
    "for i in range( 0 , len(org_cut_data) )[::-1]:\n",
    "    if Genderlist[i] == '0':\n",
    "        IDlist.pop(i)\n",
    "        Genderlist.pop(i)\n",
    "        final_cut_data.pop(i)\n",
    "        org_cut_data.pop(i)\n",
    "\n",
    "print ( \"剔除未知之后数据个数:%d\" % (len(org_cut_data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:13703 , test:5873\n"
     ]
    }
   ],
   "source": [
    "#切分数据\n",
    "\n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "  \n",
    "doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Genderlist, test_size = 0.3)  \n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(org_cut_data, Genderlist, test_size = 0.3)  \n",
    "print( \"train:%d , test:%d\" % ( len(doc_terms_train) , len(doc_terms_test) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#转换\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# 初始化TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)\n",
    "x_train = vectorizer.fit_transform(doc_terms_train)\n",
    "x_test = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "#print( x_train[0] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有待考虑：TfidfVectorizer ？= CountVectorizer + TfidfTransformer\n",
    "'''python\n",
    "#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer=TfidfTransformer()\n",
    "\n",
    "x_vtrain = vectorizer.fit_transform(doc_terms_train)\n",
    "x_vtest = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "x_train = transformer.fit_transform(x_vtrain)\n",
    "x_test = transformer.transform(x_vtest)\n",
    "\n",
    "#x_train = vectorizer.fit_transform(doc_terms_train)\n",
    "#x_test = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "allwords = vectorizer.get_feature_names()\n",
    "print( \"当前所有词汇数量%d\" % ( len(allwords) ) )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\"\"\"\n",
    "model = GradientBoostingClassifier(n_estimators=200)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "\"\"\"\n",
    "def fun_LogisticRegression( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.linear_model.logistic import LogisticRegression\n",
    "    #model = LogisticRegression()\n",
    "    model = LogisticRegression(C=2,penalty='l2')\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_MultinomialNB( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)\n",
    "    print( model )\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_BernoulliNB( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.naive_bayes import BernoulliNB  \n",
    "    model = BernoulliNB()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_KNeighborsClassifier( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_DecisionTreeClassifier( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_SVC( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694704580283\n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "0.672058573131\n",
      "0.812872467223\n"
     ]
    }
   ],
   "source": [
    "fun_LogisticRegression( x_train , y_train , x_test , y_test )\n",
    "fun_MultinomialNB( x_train , y_train , x_test , y_test )\n",
    "fun_BernoulliNB( x_train , y_train , x_test , y_test )\n",
    "#fun_SVC( x_train , y_train , x_test , y_test )\n",
    "#fun_DecisionTreeClassifier( x_train , y_train , x_test , y_test )\n",
    "#fun_KNeighborsClassifier( x_train , y_train , x_test , y_test )\n",
    "\n",
    "#0.797377830751\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   43.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=4, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.5, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l1', preprocessor=None, smooth_idf=True...vocabulary=None)), ('clf', BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'vect__max_df': (0.25, 0.5, 0.75)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)\n",
      "0.807706341677\n",
      " \t vect__max_df: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB  \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)),\n",
    "    ('clf', BernoulliNB(alpha=0.01)),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.25, 0.5, 0.75),\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV( pipeline , parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=4)\n",
    "grid.fit(doc_terms_train, y_train)\n",
    "#grid.fit(x_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "best_parameters = grid.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(' \\t %s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#predictions = grid_search.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
