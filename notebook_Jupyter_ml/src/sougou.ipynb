{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始数据20000条\n",
      "读取停用词2612条\n",
      "读取搜狗词库157202\n",
      "读取切分后数据20000条\n",
      "原有数据个数:20000\n",
      "剔除未知之后数据个数:17663\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#全局数据\n",
    "IDlist = []\n",
    "Agelist = []\n",
    "Genderlist = []\n",
    "Edulist = []\n",
    "Datalist = []\n",
    "\n",
    "Alllist = []\n",
    "\n",
    "def mycut( strCnText ):\n",
    "    words = pseg.cut(strCnText)\n",
    "    nword = []\n",
    "    for w in words:\n",
    "        #去除一些不关心的词:[c  连词][p  介词][q  量词][t  时间词][m  量词][eng  英文字母]\n",
    "        if((w.flag != 'c' and w.flag != 'p' and w.flag != 'q' and w.flag != 't'and w.flag != 'm'and w.flag != 'eng') and len(w.word)>1):\n",
    "            #通过停用词库去除一些不需要考虑的词汇\n",
    "            if w.word not in stopwords:\n",
    "                nword.append(w.word)\n",
    "            \n",
    "    return nword\n",
    "\n",
    "#开始对数据进行读取\n",
    "org_cut_data_file = '/ml/sougou/orgcut.dat'\n",
    "def create_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == True ):\n",
    "        cget = raw_input(\"File exist,cover it?(y / n)\")\n",
    "        if ( cget == 'y' ):\n",
    "            os.remove( org_cut_data_file )\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    #创建文件\n",
    "    print( \"开始创建文件.....\" )\n",
    "    fp = open( org_cut_data_file , 'w' )\n",
    "    for i in range( 0 , nRealSize ):\n",
    "    #for i in range( 0 , 10 ):\n",
    "        if ( i % 100 == 0 ):\n",
    "            print( \"%d / %d\" % ( i , nRealSize ) )\n",
    "            fp.flush()\n",
    "        fp.write( ','.join(mycut(Datalist[i])).encode('utf') + '\\n' )\n",
    "    fp.close()\n",
    "        \n",
    "#加载数据\n",
    "def load_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == False ):\n",
    "        create_org_cut_data()\n",
    "    lines = [line.strip().decode('utf', 'ignore') for line in open(org_cut_data_file).readlines()]\n",
    "    orgcutdata = []\n",
    "    for line in lines:\n",
    "        orgcutdata.append( line.split(',') )\n",
    "    return orgcutdata\n",
    "\n",
    "#获得词汇词性\n",
    "#获得词汇词性\n",
    "def GetFlag(Text):\n",
    "    try:\n",
    "        Flag = SogouDict[Text]\n",
    "    except:\n",
    "        Flag = None\n",
    "\n",
    "    return Flag\n",
    "\n",
    "'''\n",
    "N\t\t名词\n",
    "V\t\t动词\n",
    "ADJ\t\t形容词\n",
    "ADV\t\t副词\n",
    "CLAS\t量词\n",
    "ECHO\t拟声词\n",
    "STRU\t结构助词\n",
    "AUX\t\t助词\n",
    "COOR\t并列连词\n",
    "CONJ\t连词\n",
    "SUFFIX\t前缀\n",
    "PREFIX\t后缀\n",
    "PREP\t介词\n",
    "PRON\t代词\n",
    "QUES\t疑问词\n",
    "NUM\t\t数词\n",
    "IDIOM\t成语\n",
    "'''\n",
    "def IsOkWord( word ):\n",
    "    Flag = GetFlag( word )\n",
    "    if ( Flag == None ):\n",
    "        return True\n",
    "    \n",
    "    if ( (\"ECHO\" in Flag) or (\"STRU\" in Flag) or (\"AUX\" in Flag) or (\"COOR\" in Flag) or (\"CONJ\" in Flag) or (\"SUFFIX\" in Flag) or (\"PREFIX\" in Flag) ):\n",
    "        return False\n",
    "    if ( (\"PREP\" in Flag) or (\"PRON\" in Flag) or (\"QUES\" in Flag) or (\"NUM\" in Flag) ):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "#对数据进行二次处理，一方面是将数据改为空格间隔，另一方面是可以对数据进行进一步的处理\n",
    "def org_to_final_cut_data(org_cut_data):\n",
    "    cutdata = []\n",
    "    for words in org_cut_data:\n",
    "        #这里可以对words进行进一步的调整    \n",
    "        '''\n",
    "        wordslen = len( words )\n",
    "        for i in  range(0,wordslen)[::-1]:\n",
    "            if ( IsOkWord( words[i].encode(\"utf-8\") ) == False ):\n",
    "                words.pop(i)\n",
    "        '''     \n",
    "        cutdata.append( ' '.join(words) )\n",
    "        #cutdata.append( ' '.join(set(words)) )\n",
    "       \n",
    "    return cutdata\n",
    "\n",
    "\n",
    "#-------------------------开始\n",
    "#按行读取数据\n",
    "file = open('/ml/sougou/user_tag_query.2W.TRAIN')\n",
    "file_lines = file.readlines()\n",
    "file_linenum = len(file_lines)\n",
    "file.close()\n",
    "\n",
    "#age:    0：未知年龄; 1：0-18岁; 2：19-23岁; 3：24-30岁; 4：31-40岁; 5：41-50岁; 6： 51-999岁\n",
    "#gender: 0：未知1：男性2：女性\n",
    "#edu:    0：未知学历; 1：博士; 2：硕士; 3：大学生; 4：高中; 5：初中; 6：小学\n",
    "\n",
    "#通过正则表达式说去数据\n",
    "pattern = '([0-9A-Z]*)\\s([0-6]{1})\\s([0-2]{1})\\s([0-6]{1})\\s(.*)'\n",
    "\n",
    "for i in range( 0 , file_linenum ):\n",
    "    m = re.search( pattern , file_lines[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        IDlist.append(m.group(1))\n",
    "        Agelist.append(m.group(2))\n",
    "        Genderlist.append(m.group(3))\n",
    "        Edulist.append(m.group(4))\n",
    "        Datalist.append(m.group(5))\n",
    "        Alllist.append( m.group(2) + m.group(3) + m.group(4) )\n",
    "\n",
    "print( \"读取原始数据%d条\" % (len(IDlist)) )\n",
    "\n",
    "#文本处理\n",
    "#读取停用词\n",
    "stopwords = [line.strip().decode('utf-8', 'ignore') for line in open('/ml/sougou/stopwords.txt').readlines()]\n",
    "print( \"读取停用词%d条\" % (len(stopwords)) )\n",
    "\n",
    "#读取搜狗词库\n",
    "SogouDict = {}\n",
    "file = open('/ml/sougou/SogouLabDic.dic')\n",
    "SogouLabDic = file.readlines()\n",
    "SogouLabDicLen = len( SogouLabDic )\n",
    "pattern = '([^\\s]*)\\s*[0-9]*\\s*(.*)'\n",
    "for i in range( 0 , SogouLabDicLen ):\n",
    "    m = re.search( pattern , SogouLabDic[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        SogouDict[m.group(1).decode(\"gbk\",'ignore').encode(\"utf-8\")] = m.group(2)\n",
    "SogouLabDicLen = len(SogouDict)\n",
    "print( \"读取搜狗词库%d\" % SogouLabDicLen )\n",
    "\n",
    "#处理数据\n",
    "org_cut_data = load_org_cut_data()\n",
    "final_cut_data = org_to_final_cut_data( org_cut_data )\n",
    "print( \"读取切分后数据%d条\" % len(final_cut_data) )\n",
    "\n",
    "#print( final_cut_data[0] )\n",
    "\n",
    "#这里增加一个对性别位置数据的剔除工作\n",
    "print( \"原有数据个数:%d\" % (len(org_cut_data)) )\n",
    "for i in range( 0 , len(org_cut_data) )[::-1]:\n",
    "    if (Agelist[i] == '0' or  Genderlist[i] == '0' or Edulist[i] == '0'):\n",
    "    #if ( Genderlist[i] == '0' ):\n",
    "        IDlist.pop(i)\n",
    "        Agelist.pop(i)\n",
    "        Genderlist.pop(i)\n",
    "        Edulist.pop(i)\n",
    "        Alllist.pop(i)\n",
    "        final_cut_data.pop(i)\n",
    "        org_cut_data.pop(i)\n",
    "\n",
    "print ( \"剔除未知之后数据个数:%d\" % (len(org_cut_data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:12364 , test:5299\n"
     ]
    }
   ],
   "source": [
    "#切分数据\n",
    "\n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "  \n",
    "doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Genderlist, test_size = 0.3)  \n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Agelist, test_size = 0.3)\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Edulist, test_size = 0.3)\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Alllist, test_size = 0.3)\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(org_cut_data, Genderlist, test_size = 0.3)  \n",
    "print( \"train:%d , test:%d\" % ( len(doc_terms_train) , len(doc_terms_test) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#转换\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#********FOR None\n",
    "vectorizer = TfidfVectorizer( max_features = 5000 )\n",
    "print( vectorizer )\n",
    "'''\n",
    "#********FOR LogisticRegression\n",
    "vectorizerL = TfidfVectorizer(max_df=0.5 , max_features = None , ngram_range = (1, 2) , norm = 'l2' ,use_idf =True )\n",
    "\n",
    "#********FOR MultinomialNB\n",
    "vectorizerM = TfidfVectorizer(max_df = 0.5 , max_features = 10000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "\n",
    "#********FOR BernoulliNB\n",
    "vectorizerB = TfidfVectorizer(max_df=0.5 , max_features=10000 , ngram_range=(1, 1) , norm='l1', use_idf=True)\n",
    "'''\n",
    "'''\n",
    "x_train = vectorizer.fit_transform(doc_terms_train)\n",
    "x_test = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "x_trainL = vectorizerL.fit_transform(doc_terms_train)\n",
    "x_testL = vectorizerL.transform(doc_terms_test)\n",
    "\n",
    "x_trainM = vectorizerM.fit_transform(doc_terms_train)\n",
    "x_testM = vectorizerM.transform(doc_terms_test)\n",
    "\n",
    "x_trainB = vectorizerB.fit_transform(doc_terms_train)\n",
    "x_testB = vectorizerB.transform(doc_terms_test)\n",
    "'''\n",
    "\n",
    "#vectorizer = CountVectorizer()\n",
    "\n",
    "#x_vtrain = vectorizer.fit_transform(doc_terms_train)\n",
    "#x_vtest = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "#print( vectorizer )\n",
    "\n",
    "print( \"ok\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有待考虑：TfidfVectorizer ？= CountVectorizer + TfidfTransformer\n",
    "'''python\n",
    "#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer=TfidfTransformer()\n",
    "\n",
    "x_vtrain = vectorizer.fit_transform(doc_terms_train)\n",
    "x_vtest = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "x_train = transformer.fit_transform(x_vtrain)\n",
    "x_test = transformer.transform(x_vtest)\n",
    "\n",
    "#x_train = vectorizer.fit_transform(doc_terms_train)\n",
    "#x_test = vectorizer.transform(doc_terms_test)\n",
    "\n",
    "allwords = vectorizer.get_feature_names()\n",
    "print( \"当前所有词汇数量%d\" % ( len(allwords) ) )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\"\"\"\n",
    "model = GradientBoostingClassifier(n_estimators=200)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "\"\"\"\n",
    "#0.802315681934\n",
    "def fun_LogisticRegression( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.linear_model.logistic import LogisticRegression\n",
    "    #model = LogisticRegression()\n",
    "    model = LogisticRegression(C=2,penalty='l2')\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "#0.801464328282\n",
    "def fun_MultinomialNB( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha = 0.01)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "\n",
    "#0.811510301379\n",
    "def fun_BernoulliNB( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.naive_bayes import BernoulliNB  \n",
    "    model = BernoulliNB(alpha=0.01)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_KNeighborsClassifier( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_DecisionTreeClassifier( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_SVC( x_train , y_train , x_test , y_test ):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    print( precision_score(y_test, predictions, average='micro')   )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No param:\n",
      "0.797697678807\n",
      "0.805246272882\n",
      "0.812983581808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint( \"TfidfVectorizer For LogisticRegression:\" )\\nfun_LogisticRegression( x_trainL , y_train , x_testL , y_test )\\nfun_MultinomialNB( x_trainL , y_train , x_testL , y_test )\\nfun_BernoulliNB( x_trainL , y_train , x_testL , y_test )\\n\\nprint( \"TfidfVectorizer For MultinomialNB:\" )\\nfun_LogisticRegression( x_trainM , y_train , x_testM , y_test )\\nfun_MultinomialNB( x_trainM , y_train , x_testM , y_test )\\nfun_BernoulliNB( x_trainM , y_train , x_testM , y_test )\\n\\nprint( \"TfidfVectorizer For BernoulliNB:\" )\\nfun_LogisticRegression( x_trainB , y_train , x_testB , y_test )\\nfun_MultinomialNB( x_trainB , y_train , x_testB , y_test )\\nfun_BernoulliNB( x_trainB , y_train , x_testB , y_test )\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print( \"No param:\" )\n",
    "fun_LogisticRegression( x_train , y_train , x_test , y_test )\n",
    "fun_MultinomialNB( x_train , y_train , x_test , y_test )\n",
    "fun_BernoulliNB( x_train , y_train , x_test , y_test )\n",
    "'''\n",
    "print( \"TfidfVectorizer For LogisticRegression:\" )\n",
    "fun_LogisticRegression( x_trainL , y_train , x_testL , y_test )\n",
    "fun_MultinomialNB( x_trainL , y_train , x_testL , y_test )\n",
    "fun_BernoulliNB( x_trainL , y_train , x_testL , y_test )\n",
    "\n",
    "print( \"TfidfVectorizer For MultinomialNB:\" )\n",
    "fun_LogisticRegression( x_trainM , y_train , x_testM , y_test )\n",
    "fun_MultinomialNB( x_trainM , y_train , x_testM , y_test )\n",
    "fun_BernoulliNB( x_trainM , y_train , x_testM , y_test )\n",
    "\n",
    "print( \"TfidfVectorizer For BernoulliNB:\" )\n",
    "fun_LogisticRegression( x_trainB , y_train , x_testB , y_test )\n",
    "fun_MultinomialNB( x_trainB , y_train , x_testB , y_test )\n",
    "fun_BernoulliNB( x_trainB , y_train , x_testB , y_test )\n",
    "'''\n",
    "\n",
    "#fun_SVC( x_train , y_train , x_test , y_test )\n",
    "#fun_DecisionTreeClassifier( x_train , y_train , x_test , y_test )\n",
    "#fun_KNeighborsClassifier( x_train , y_train , x_test , y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   55.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=4, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'vect__max_features': (10000, 15000, None)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)\n",
      "0.804674862504\n",
      " \t vect__max_features: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#    ('vect', TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)),\n",
    "#    ('clf', BernoulliNB(alpha=0.01)),\n",
    "    \n",
    "#    ('vect', TfidfVectorizer(max_df = 0.5 , max_features = 10000 , ngram_range = (1, 2) , norm = 'l2')),\n",
    "#    ('clf', MultinomialNB(alpha = 0.01))\n",
    "    \n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(C=2,penalty='l2'))\n",
    "])\n",
    "parameters = {\n",
    "\n",
    "    #'vect__max_df': (0.25, 0.5, 0.75),\n",
    "    #'vect__stop_words': ('english', None ),\n",
    "    'vect__max_features': (  10000,15000, None ),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    #'vect__use_idf': ( True , False ),\n",
    "    #'vect__norm': ('l1', 'l2'),\n",
    "    \n",
    "    #'clf__penalty': ('l1', 'l2'),\n",
    "    #'clf__C': (0.01, 0.1, 1, 2 , 3)\n",
    "    #'clf__alpha': ( 0.1, 0.01, 0.001, 0.0001)  \n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV( pipeline , parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=4)\n",
    "grid.fit(doc_terms_train, y_train)\n",
    "#grid.fit(x_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "best_parameters = grid.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(' \\t %s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#predictions = grid_search.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
