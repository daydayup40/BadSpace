{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始数据20000\n",
      "读取停用词2612\n",
      "读取搜狗词库157202\n",
      "读取切分后数据20000条\n",
      "原有数据个数:20000\n",
      "剔除未知之后数据个数:17663\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "org_data_file_name = '/ml/sougou/user_tag_query.10W.TRAIN'\n",
    "org_cut_data_file = '/ml/sougou/orgcut_10W.dat'\n",
    "modelname = \"/ml/sougou/w2v_model_10W.model\"\n",
    "'''\n",
    "org_data_file_name = '/ml/sougou/user_tag_query.2W.TRAIN'\n",
    "org_cut_data_file = '/ml/sougou/orgcut.dat'\n",
    "modelname = \"/ml/sougou/w2v_model.model\"\n",
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#全局数据\n",
    "IDlist = []\n",
    "Agelist = []\n",
    "Genderlist = []\n",
    "Edulist = []\n",
    "Datalist = []\n",
    "\n",
    "Alllist = []\n",
    "\n",
    "def mycut( strCnText ):\n",
    "    words = pseg.cut(strCnText)\n",
    "    nword = []\n",
    "    #去除一些不关心的词:[c  连词][p  介词][q  量词][t  时间词][m  量词][eng  英文字母]\n",
    "    notneeds = [ 'c' , 'p' , 'q' , 't' , 'm' , 'eng' ]\n",
    "    needs = [ 'n' , 'a' , 'ad' , 'an' , 'd' , 'i' , 'v' ]\n",
    "    for w in words:\n",
    "        #fp.write( w.word.encode('utf') + \" \" + w.flag.encode('utf') + '\\n' )\n",
    "        if ( (w.flag not in notneeds) and (len(w.word)>1) ):\n",
    "        #if ( (w.flag in needs) and (len(w.word)>1) ):\n",
    "            #通过停用词库去除一些不需要考虑的词汇\n",
    "            if w.word not in stopwords:\n",
    "                nword.append(w.word)\n",
    "            \n",
    "    return nword\n",
    "\n",
    "#开始对数据进行读取\n",
    "#org_cut_data_file = '/ml/sougou/orgcut.dat'\n",
    "def create_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == True ):\n",
    "        cget = raw_input(\"File exist,cover it?(y / n)\")\n",
    "        if ( cget == 'y' ):\n",
    "            os.remove( org_cut_data_file )\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    #创建文件\n",
    "    print( \"开始创建文件.....\" )\n",
    "    fp = open( org_cut_data_file , 'w' )\n",
    "    for i in range( 0 , len(Datalist) ):\n",
    "    #for i in range( 0 , 10 ):\n",
    "        if ( i % 100 == 0 ):\n",
    "            print( \"%d / %d\" % ( i , len(Datalist) ) )\n",
    "            fp.flush()\n",
    "        fp.write( ','.join(mycut(Datalist[i])).encode('utf') + '\\n' )\n",
    "    fp.close()\n",
    "        \n",
    "#加载数据\n",
    "def load_org_cut_data():\n",
    "    if ( os.path.exists(org_cut_data_file) == False ):\n",
    "        create_org_cut_data()\n",
    "    lines = [line.strip().decode('utf', 'ignore') for line in open(org_cut_data_file).readlines()]\n",
    "    orgcutdata = []\n",
    "    for line in lines:\n",
    "        orgcutdata.append( line.split(',') )\n",
    "    return orgcutdata\n",
    "\n",
    "#获得词汇词性\n",
    "def GetFlag(Text):\n",
    "    try:\n",
    "        Flag = SogouDict[Text]\n",
    "    except:\n",
    "        Flag = None\n",
    "\n",
    "    return Flag\n",
    "\n",
    "'''\n",
    "N\t\t名词\n",
    "V\t\t动词\n",
    "ADJ\t\t形容词\n",
    "ADV\t\t副词\n",
    "CLAS\t量词\n",
    "ECHO\t拟声词\n",
    "STRU\t结构助词\n",
    "AUX\t\t助词\n",
    "COOR\t并列连词\n",
    "CONJ\t连词\n",
    "SUFFIX\t前缀\n",
    "PREFIX\t后缀\n",
    "PREP\t介词\n",
    "PRON\t代词\n",
    "QUES\t疑问词\n",
    "NUM\t\t数词\n",
    "IDIOM\t成语\n",
    "'''\n",
    "def IsOkWord( word ):\n",
    "    Flag = GetFlag( word )\n",
    "    if ( Flag == None ):\n",
    "        return True\n",
    "    #(\"ECHO\" in Flag) or  \n",
    "    if (  (\"STRU\" in Flag) or (\"AUX\" in Flag) or (\"COOR\" in Flag) \\\n",
    "        or (\"CONJ\" in Flag) or (\"SUFFIX\" in Flag) or (\"PREFIX\" in Flag) or (\"CLAS\" in Flag) \\\n",
    "        or (\"PREP\" in Flag) or (\"PRON\" in Flag) or (\"QUES\" in Flag) or (\"NUM\" in Flag)):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "#对数据进行二次处理，一方面是将数据改为空格间隔，另一方面是可以对数据进行进一步的处理\n",
    "def org_to_final_cut_data(org_cut_data):\n",
    "    cutdata = []\n",
    "    for words in org_cut_data:\n",
    "        #这里可以对words进行进一步的调整    \n",
    "\n",
    "        wordslen = len( words )\n",
    "        for i in  range(0,wordslen)[::-1]:\n",
    "            if ( IsOkWord( words[i].encode(\"utf-8\") ) == False ):\n",
    "                words.pop(i)\n",
    "\n",
    "        cutdata.append( ' '.join(words) )\n",
    "        #cutdata.append( ' '.join(set(words)) )\n",
    "       \n",
    "    return cutdata\n",
    "\n",
    "\n",
    "#-------------------------开始\n",
    "#按行读取数据\n",
    "#file = open('/ml/sougou/user_tag_query.2W.TRAIN')\n",
    "file = open(org_data_file_name)\n",
    "file_lines = file.readlines()\n",
    "file_linenum = len(file_lines)\n",
    "file.close()\n",
    "\n",
    "#age:    0：未知年龄; 1：0-18岁; 2：19-23岁; 3：24-30岁; 4：31-40岁; 5：41-50岁; 6： 51-999岁\n",
    "#gender: 0：未知1：男性2：女性\n",
    "#edu:    0：未知学历; 1：博士; 2：硕士; 3：大学生; 4：高中; 5：初中; 6：小学\n",
    "\n",
    "#通过正则表达式说去数据\n",
    "pattern = '([0-9A-Z]*)\\s([0-6]{1})\\s([0-2]{1})\\s([0-6]{1})\\s(.*)'\n",
    "\n",
    "for i in range( 0 , file_linenum ):\n",
    "    m = re.search( pattern , file_lines[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        IDlist.append(m.group(1))\n",
    "        Agelist.append(m.group(2))\n",
    "        Genderlist.append(m.group(3))\n",
    "        Edulist.append(m.group(4))\n",
    "        Datalist.append(m.group(5))\n",
    "        Alllist.append( m.group(2) + m.group(3) + m.group(4) )\n",
    "\n",
    "print( \"读取原始数据%d\" % (len(IDlist)) )\n",
    "\n",
    "#文本处理\n",
    "#读取停用词\n",
    "stopwords = [line.strip().decode('utf-8', 'ignore') for line in open('/ml/sougou/stopwords.txt').readlines()]\n",
    "print( \"读取停用词%d\" % (len(stopwords)) )\n",
    "\n",
    "#读取搜狗词库\n",
    "\n",
    "SogouDict = {}\n",
    "file = open('/ml/sougou/SogouLabDic.dic')\n",
    "SogouLabDic = file.readlines()\n",
    "SogouLabDicLen = len( SogouLabDic )\n",
    "pattern = '([^\\s]*)\\s*[0-9]*\\s*(.*)'\n",
    "for i in range( 0 , SogouLabDicLen ):\n",
    "    m = re.search( pattern , SogouLabDic[i] )\n",
    "    if m == None:\n",
    "        print( '[ERROR]bad lines:' , i )\n",
    "        break\n",
    "    else:\n",
    "        SogouDict[m.group(1).decode(\"gbk\",'ignore').encode(\"utf-8\")] = m.group(2)\n",
    "SogouLabDicLen = len(SogouDict)\n",
    "print( \"读取搜狗词库%d\" % SogouLabDicLen )\n",
    "\n",
    "\n",
    "#处理数据\n",
    "org_cut_data = load_org_cut_data()\n",
    "final_cut_data = org_to_final_cut_data( org_cut_data )\n",
    "print( \"读取切分后数据%d条\" % len(final_cut_data) )\n",
    "\n",
    "#print( final_cut_data[0] )\n",
    "\n",
    "#这里增加一个对性别位置数据的剔除工作\n",
    "\n",
    "print( \"原有数据个数:%d\" % (len(org_cut_data)) )\n",
    "for i in range( 0 , len(org_cut_data) )[::-1]:\n",
    "    if (Agelist[i] == '0' or  Genderlist[i] == '0' or Edulist[i] == '0'):\n",
    "    #if ( Genderlist[i] == '0' ):\n",
    "        IDlist.pop(i)\n",
    "        Agelist.pop(i)\n",
    "        Genderlist.pop(i)\n",
    "        Edulist.pop(i)\n",
    "        Alllist.pop(i)\n",
    "        final_cut_data.pop(i)\n",
    "        org_cut_data.pop(i)\n",
    "\n",
    "print ( \"剔除未知之后数据个数:%d\" % (len(org_cut_data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:14130 test:3533\n"
     ]
    }
   ],
   "source": [
    "#切分数据\n",
    "\n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Genderlist, test_size = 0.3)  \n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Agelist, test_size = 0.2)\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, Edulist, test_size = 0.2)\n",
    "doc_terms_train_spl, doc_terms_test_spl, y_train_spl , y_test_spl = train_test_split(final_cut_data, Alllist, test_size = 0.2)\n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(org_cut_data, Genderlist, test_size = 0.3)  \n",
    "#doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(final_cut_data, TargetList, test_size = 0.2)\n",
    "\n",
    "y_train_a = []\n",
    "y_train_g = []\n",
    "y_train_e = []\n",
    "\n",
    "for i in range( 0 , len(y_train_spl) ):\n",
    "    y_train_a.append( y_train_spl[i][0] )\n",
    "    y_train_g.append( y_train_spl[i][1] )\n",
    "    y_train_e.append( y_train_spl[i][2] )\n",
    "\n",
    "y_test_a = []\n",
    "y_test_g = []\n",
    "y_test_e = []\n",
    "    \n",
    "for i in range( 0 , len(y_test_spl) ):\n",
    "    y_test_a.append( y_test_spl[i][0] )\n",
    "    y_test_g.append( y_test_spl[i][1] )\n",
    "    y_test_e.append( y_test_spl[i][2] )\n",
    "\n",
    "y_train = {'a': y_train_a, 'g': y_train_g, 'e': y_train_e}  \n",
    "y_test = {'a': y_test_a, 'g': y_test_g, 'e': y_test_e}\n",
    "\n",
    "doc_terms_train = doc_terms_train_spl\n",
    "doc_terms_test = doc_terms_test_spl\n",
    "\n",
    "'''\n",
    "doc_terms_train_a = []\n",
    "doc_terms_train_g = []\n",
    "doc_terms_train_e = []\n",
    "for i in range( 0 , len(doc_terms_train_spl) ):\n",
    "    doc_terms_train_a.append( doc_terms_train_spl[i] )\n",
    "    doc_terms_train_g.append( doc_terms_train_spl[i] )\n",
    "    doc_terms_train_e.append( doc_terms_train_spl[i] )\n",
    "    \n",
    "doc_terms_train = {'a': doc_terms_train_a, 'g': doc_terms_train_g, 'e': doc_terms_train_e}  \n",
    "doc_terms_test = doc_terms_test_spl\n",
    "\n",
    "for flag in ['a','g','e']:\n",
    "    print( \"train_\" + flag + \"原有数据个数:%d\" % (len(y_train[flag])) )\n",
    "    for i in range( 0 , len(y_train[flag]) )[::-1]:\n",
    "        if ( y_train[flag][i] == '0' ):\n",
    "            y_train[flag].pop(i)\n",
    "            doc_terms_train[flag].pop(i)\n",
    "    print( \"train_\" + flag + \"现有数据个数:%d\" % (len(y_train[flag])) )\n",
    "\n",
    "print( \"test原有数据个数:%d\" % (len(y_test[flag])) )\n",
    "for i in range( 0 , len(doc_terms_test) )[::-1]:\n",
    "    if ( y_test['a'][i] == '0' or  y_test['g'][i] == '0' or y_test['e'][i] == '0'):\n",
    "        y_test['a'].pop(i)\n",
    "        y_test['g'].pop(i)\n",
    "        y_test['e'].pop(i)\n",
    "        doc_terms_test.pop(i)\n",
    "print( \"test现有数据个数:%d\" % (len(doc_terms_test)) )\n",
    "'''\n",
    "print( \"train:%d test:%d\" % (len(doc_terms_train) , len(doc_terms_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=46336, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from gensim.models import word2vec\n",
    "\n",
    "w2vsize = 300\n",
    "\n",
    "def w2v_transform( model , data ):\n",
    "    res=np.zeros((len(data),w2vsize))\n",
    "    for i,line in enumerate(data):\n",
    "        terms=line.split()\n",
    "        count=0\n",
    "        for j,term in enumerate(terms):\n",
    "            try:#---try失败说明X中有单词不在model中，训练的时候model的模型是min_count的 忽略了一部分单词\n",
    "                count += 1\n",
    "                res[i]+=np.array(model[term])\n",
    "            except:\n",
    "                1 == 1\n",
    "        if count!=0:\n",
    "            res[i]=res[i]/float(count) # 求均值\n",
    "    return res\n",
    "\n",
    "def w2v_loadmodel():\n",
    "    model = word2vec.Word2Vec.load(modelname)\n",
    "    return model\n",
    "\n",
    "def w2v_createmodel( data ):\n",
    "    print '转换数据...'\n",
    "    datalist = []\n",
    "    for i in range(0,len(data)):\n",
    "        datalist.append( data[i].replace( ',' , ' ' ).split() )\n",
    "    \n",
    "    print '开始训练model...'\n",
    "    start = time.clock()\n",
    "    model = word2vec.Word2Vec(datalist , size=w2vsize, window=100,workers=48)  # 训练模型; 注意参数window 对结果有影响 一般5-100\n",
    "    end = time.clock()\n",
    "    \n",
    "    print \"训练完毕: %f s\" % (end - start)\n",
    "    model.save( modelname )\n",
    "    print '保存完毕'\n",
    "    \n",
    "#w2v_createmodel( final_cut_data )\n",
    "model = w2v_loadmodel()\n",
    "print model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性别数据矩阵化:\n",
      "max_features = 100000\n",
      "年龄数据矩阵化:\n",
      "max_features = 130000\n",
      "学历数据矩阵化:\n",
      "max_features = 170000\n",
      "word2vec 模式转化:\n",
      "word2vec test 模式转化:\n",
      "size = 300\n"
     ]
    }
   ],
   "source": [
    "#转换\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def mytransform( doc_train , doc_test ):\n",
    "#    vectorizer_g = TfidfVectorizer(max_df = 0.75 , max_features = 90000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "#    vectorizer_a = TfidfVectorizer(max_df = 0.75 , max_features = 130000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "#    vectorizer_e = TfidfVectorizer(max_df = 0.75 , max_features = 170000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "\n",
    "    vectorizer_g = TfidfVectorizer(max_df = 0.75 , max_features = 100000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "    vectorizer_a = TfidfVectorizer(max_df = 0.75 , max_features = 130000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "    vectorizer_e = TfidfVectorizer(max_df = 0.75 , max_features = 170000 , ngram_range = (1, 2) , norm = 'l2')\n",
    "\n",
    "    print '性别数据矩阵化:'\n",
    "    x_train_g = vectorizer_g.fit_transform(doc_terms_train)\n",
    "    x_test_g = vectorizer_g.transform(doc_terms_test)\n",
    "    print( \"max_features = %d\" % ( len(vectorizer_g.get_feature_names()) ) )\n",
    "    \n",
    "    print '年龄数据矩阵化:'\n",
    "    x_train_a = vectorizer_a.fit_transform(doc_terms_train)\n",
    "    x_test_a = vectorizer_a.transform(doc_terms_test)\n",
    "    print( \"max_features = %d\" % ( len(vectorizer_a.get_feature_names()) ) )\n",
    "    \n",
    "    print '学历数据矩阵化:'\n",
    "    x_train_e = vectorizer_e.fit_transform(doc_terms_train)\n",
    "    x_test_e = vectorizer_e.transform(doc_terms_test)\n",
    "    print( \"max_features = %d\" % ( len(vectorizer_e.get_feature_names()) ) )\n",
    "    \n",
    "    print 'word2vec 模式转化:'\n",
    "    x_trainW = w2v_transform( model , doc_terms_train )\n",
    "    print 'word2vec test 模式转化:'\n",
    "    x_testW = w2v_transform( model , doc_terms_test )\n",
    "    print( \"size = 300\" )\n",
    "    \n",
    "    return x_train_g , x_test_g , x_train_a , x_test_a , x_train_e , x_test_e , x_trainW , x_testW\n",
    "\n",
    "x_train = {}\n",
    "x_test = {}\n",
    "x_train['g'], x_test['g'] ,x_train['a'] ,x_test['a'] ,x_train['e'] ,x_test['e'] ,x_train['w'] ,x_test['w'] = \\\n",
    "mytransform( doc_terms_train , doc_terms_test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "random_rate = 8240\n",
    "\n",
    "def clf_LogisticRegression( x_train , y_train ):\n",
    "    from sklearn.linear_model.logistic import LogisticRegression\n",
    "    model = LogisticRegression(C=2,penalty='l2')\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_MultinomialNB( x_train , y_train ):\n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_BernoulliNB( x_train , y_train ):\n",
    "    from sklearn.naive_bayes import BernoulliNB  \n",
    "    model = BernoulliNB(alpha=0.01)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_SGDClassifier( x_train , y_train ):\n",
    "    from sklearn.linear_model import SGDClassifier  \n",
    "    model = SGDClassifier(alpha=5e-05)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_KNeighborsClassifier( x_train , y_train ):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_SVC( x_train , y_train ):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC( kernel = 'linear' )\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_DecisionTreeClassifier( x_train , y_train ):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_LinearSVC( x_train , y_train ):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    model = LinearSVC(C=0.1, random_state=random_rate)\n",
    "    #model = LinearSVC()\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_RidgeClassifier( x_train , y_train ):\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    model = RidgeClassifier(alpha=0.5)\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "def clf_PassiveAggressiveClassifier( x_train , y_train ):\n",
    "    from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "    model = PassiveAggressiveClassifier( C = 0.9 , loss = 'hinge' , n_iter = 20 )\n",
    "    clf = model.fit(x_train , y_train )\n",
    "    return clf\n",
    "\n",
    "'''=========================================================================='''\n",
    "#0.802315681934\n",
    "def fun_LogisticRegression( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_LogisticRegression(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return precision_score(y_test, predictions, average='micro')\n",
    "    \n",
    "#0.801464328282\n",
    "def fun_MultinomialNB( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_MultinomialNB(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return precision_score(y_test, predictions, average='micro')\n",
    "\n",
    "#0.811510301379\n",
    "def fun_BernoulliNB( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_BernoulliNB(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return precision_score(y_test, predictions, average='micro')\n",
    "    \n",
    "def fun_SGDClassifier( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_SGDClassifier(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_KNeighborsClassifier( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_KNeighborsClassifier(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "\n",
    "def fun_SVC( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_SVC(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "\n",
    "def fun_DecisionTreeClassifier( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_DecisionTreeClassifier(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "\n",
    "def fun_LinearSVC( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_LinearSVC(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "    \n",
    "def fun_RidgeClassifier( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_RidgeClassifier(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n",
    "\n",
    "def fun_PassiveAggressiveClassifier( x_train , y_train , x_test , y_test ):\n",
    "    clf = clf_PassiveAggressiveClassifier(x_train , y_train )\n",
    "    predictions = clf.predict( x_test )\n",
    "    return( precision_score(y_test, predictions, average='micro')   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote:\n",
      "mod = g\n",
      "mod = a\n",
      "mod = e\n",
      "0.814605 0.580810 0.607982 0.667799\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "def sub_vote( mod , *lista ):\n",
    "    dic = {}\n",
    "    for i in range( 0 , len(lista) ):\n",
    "        try:\n",
    "            dic[lista[i]] = dic[lista[i]] + 1\n",
    "        except:\n",
    "            dic[lista[i]] = 1\n",
    "\n",
    "    rec = 0\n",
    "    pop = []\n",
    "    for i in range( 0 , len(lista) ):\n",
    "        try:\n",
    "            item = max(dic.items(), key=lambda x: x[1])\n",
    "            if ( rec == 0 ):\n",
    "                rec = item[1]\n",
    "                pop.append( item[0] )\n",
    "                dic.pop( item[0] )\n",
    "            elif ( rec == item[1] ):\n",
    "                pop.append( item[0] )\n",
    "                dic.pop( item[0] )\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    if mod == 'g':\n",
    "        if len(pop) != 1:\n",
    "            return '2'\n",
    "    elif mod == 'a':\n",
    "        if len(pop) != 1:\n",
    "            return max( pop )\n",
    "    elif mod == 'e':\n",
    "        if len(pop) != 1:\n",
    "            return min( pop )\n",
    "\n",
    "    return choice(pop)\n",
    "\n",
    "def sub_vote_a( *list ):\n",
    "    sum = 0\n",
    "    for i in range( 0 , len(list) ):\n",
    "        sum += int(list[i])\n",
    "    return str( int (round( sum / float(len(list)) ) ) )\n",
    "\n",
    "\n",
    "def fun_vote( xtrain , ytrain , xtest , ytest , mod ):\n",
    "    print \"mod = \" + mod\n",
    "    pre_LR = clf_LogisticRegression( xtrain[mod] , ytrain[mod] ).predict( xtest[mod] )\n",
    "    #pre_MB = clf_MultinomialNB( xtrain[mod] , ytrain[mod] ).predict( xtest[mod] )\n",
    "    pre_BB = clf_BernoulliNB( xtrain[mod] , ytrain[mod] ).predict( xtest[mod] )\n",
    "    pre_SG = clf_SGDClassifier( xtrain[mod] , ytrain[mod] ).predict( xtest[mod] )\n",
    "    pre_LS = clf_LinearSVC( xtrain[mod] , ytrain[mod] ).predict( xtest[mod] )\n",
    "    \n",
    "    \n",
    "    pre_LRW = clf_LogisticRegression( xtrain['w'] , ytrain[mod] ).predict( xtest['w'] )\n",
    "    pre_RCW = clf_RidgeClassifier( xtrain['w'] , ytrain[mod] ).predict( xtest['w'] )\n",
    "    #pre_BBW = clf_BernoulliNB( x_trainW , ytrain ).predict( x_testW )\n",
    "    #pre_SGW = clf_SGDClassifier( x_trainW , ytrain ).predict( x_testW )\n",
    "    '''\n",
    "    0.837279 0.610938 0.632635 0.693617+\n",
    "    0.848579 0.611899 0.636307 0.698928+\n",
    "    MultinomialNB:\n",
    "    0.834002 0.573366 0.595909 0.667759\n",
    "    BernoulliNB:\n",
    "    0.832533 0.571162 0.582462 0.662052+\n",
    "    0.822815 0.539579 0.519295 0.627229\n",
    "    SGDClassifier:\n",
    "    0.844454 0.596926 0.622521 0.687967+\n",
    "    0.803435 0.571106 0.578564 0.651035\n",
    "    LinearSVC:\n",
    "    0.841968 0.609469 0.632861 0.694766++\n",
    "    RidgeClassifier:\n",
    "    0.805469 0.574326 0.598960 0.659585\n",
    "    0.846545 0.600542 0.632465 0.693184++\n",
    "    '''\n",
    "    predictions = []\n",
    "\n",
    "    #随机投票\n",
    "    for i in range( 0 , len(pre_LR) ):\n",
    "        predictions.append( sub_vote( mod , pre_LR[i],  pre_BB[i] , pre_SG[i] ,pre_LS[i] ,\\\n",
    "                                     pre_LRW[i] ,  pre_RCW[i] ,pre_LS[i] ,  pre_RCW[i] ) )\n",
    "        #predictions.append( sub_vote( mod , pre_LR[i],  pre_SG[i] , pre_LRW[i] , pre_LS[i] , pre_RCW[i]) )\n",
    "\n",
    "    return precision_score(ytest[mod], predictions, average='micro')\n",
    "\n",
    "print( \"vote:\" )\n",
    "v_g = fun_vote( x_train , y_train , x_test , y_test , 'g' )\n",
    "v_a = fun_vote( x_train , y_train , x_test , y_test , 'a' )\n",
    "v_e = fun_vote( x_train , y_train , x_test , y_test , 'e' )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print 'over'\n",
    "\n",
    "#0.834701 0.577979 0.613360 0.675347\n",
    "#0.847438 0.589018 0.612794 0.683083\n",
    "#0.847155 0.587603 0.611378 0.682045\n",
    "\n",
    "#0.837815 0.584489 0.610812 0.677705\n",
    "#0.837532 0.585055 0.610246 0.677611\n",
    "\n",
    "#0.832720 0.584772 0.603170 0.673554\n",
    "#0.832437 0.581659 0.602887 0.672328\n",
    "\n",
    "#0.845740 0.591848 0.603736 0.680442\n",
    "\n",
    "\n",
    "#0.852139 0.616928 0.640375 0.703147\n",
    "\n",
    "#0.852252 0.610769 0.633934 0.698985\n",
    "\n",
    "#0.850896 0.614724 0.635968 0.700529\n",
    "\n",
    "#0.850444 0.614950 0.635855 0.700416\n",
    "\n",
    "#无贝叶斯\n",
    "#0.844511 0.610486 0.635573 0.696857\n",
    "\n",
    "#+BernoulliNB\n",
    "#0.849370 0.616080 0.637550 0.701000\n",
    "\n",
    "#+MultinomialNB\n",
    "#0.849596 0.615119 0.637211 0.700642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "0.800170 0.566657 0.597792 0.654873\n",
      "0.809793 0.570337 0.605151 0.661761\n",
      "MultinomialNB:\n",
      "0.808944 0.569205 0.578545 0.652231\n",
      "BernoulliNB:\n",
      "0.821964 0.572884 0.579677 0.658175\n",
      "0.782055 0.519389 0.488820 0.596754\n",
      "SGDClassifier:\n",
      "0.794509 0.558166 0.580810 0.644495\n",
      "0.752335 0.481461 0.534956 0.589584\n",
      "LinearSVC:\n",
      "0.800453 0.562129 0.591565 0.651382\n",
      "RidgeClassifier:\n",
      "0.781489 0.549958 0.574866 0.635437\n",
      "0.806963 0.562695 0.598924 0.656194\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "print( \"LogisticRegression:\" )\n",
    "v_g = fun_LogisticRegression( x_train['g'] , y_train['g'] , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_LogisticRegression( x_train['a'] , y_train['a'] , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_LogisticRegression( x_train['e'] , y_train['e'] , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "v_g = fun_LogisticRegression( x_train['w'] , y_train['g'] , x_test['w'] , y_test['g'] )\n",
    "v_a = fun_LogisticRegression( x_train['w'] , y_train['a'] , x_test['w'] , y_test['a'] )\n",
    "v_e = fun_LogisticRegression( x_train['w'] , y_train['e'] , x_test['w'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print( \"MultinomialNB:\" )\n",
    "v_g = fun_MultinomialNB( x_train['g'] , y_train['g'] , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_MultinomialNB( x_train['a'] , y_train['a'] , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_MultinomialNB( x_train['e'] , y_train['e'] , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print( \"BernoulliNB:\" )\n",
    "v_g = fun_BernoulliNB( x_train['g'] , y_train['g'] , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_BernoulliNB( x_train['a'] , y_train['a'] , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_BernoulliNB( x_train['e'] , y_train['e'] , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "v_g = fun_BernoulliNB( x_train['w'] , y_train['g'] , x_test['w'] , y_test['g'] )\n",
    "v_a = fun_BernoulliNB( x_train['w'] , y_train['a'] , x_test['w'] , y_test['a'] )\n",
    "v_e = fun_BernoulliNB( x_train['w'] , y_train['e'] , x_test['w'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print( \"SGDClassifier:\" )\n",
    "v_g = fun_SGDClassifier( x_train['g'] , y_train['g'] , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_SGDClassifier( x_train['a'] , y_train['a'] , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_SGDClassifier( x_train['e'] , y_train['e'] , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "v_g = fun_SGDClassifier( x_train['w'] , y_train['g'] , x_test['w'] , y_test['g'] )\n",
    "v_a = fun_SGDClassifier( x_train['w'] , y_train['a'] , x_test['w'] , y_test['a'] )\n",
    "v_e = fun_SGDClassifier( x_train['w'] , y_train['e'] , x_test['w'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print( \"LinearSVC:\" )\n",
    "v_g = fun_LinearSVC( x_train['g'] , y_train['g'] , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_LinearSVC( x_train['a'] , y_train['a'] , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_LinearSVC( x_train['e'] , y_train['e'] , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "\n",
    "print( \"RidgeClassifier:\" )\n",
    "v_g = fun_RidgeClassifier( x_train['g'] , y_train_g , x_test['g'] , y_test['g'] )\n",
    "v_a = fun_RidgeClassifier( x_train['a'] , y_train_a , x_test['a'] , y_test['a'] )\n",
    "v_e = fun_RidgeClassifier( x_train['e'] , y_train_e , x_test['e'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "v_g = fun_RidgeClassifier( x_train['w'] , y_train_g , x_test['w'] , y_test['g'] )\n",
    "v_a = fun_RidgeClassifier( x_train['w'] , y_train_a , x_test['w'] , y_test['a'] )\n",
    "v_e = fun_RidgeClassifier( x_train['w'] , y_train_e , x_test['w'] , y_test['e'] )\n",
    "print( \"%f %f %f %f\" % ( v_g , v_a , v_e , (v_g + v_a + v_e) / 3.0 ) )\n",
    "\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#使用集合\n",
    "LogisticRegression:\n",
    "0.828191 0.580243 0.604585 0.671007\n",
    "0.820549 0.590150 0.607982 0.672894\n",
    "MultinomialNB:\n",
    "0.816020 0.569771 0.582225 0.656005\n",
    "BernoulliNB:\n",
    "0.819700 0.567506 0.577413 0.654873\n",
    "0.789414 0.529012 0.476083 0.598170\n",
    "SGDClassifier:\n",
    "0.815737 0.572035 0.595811 0.661194\n",
    "0.754599 0.522219 0.550241 0.609020\n",
    "LinearSVC:\n",
    "0.828757 0.574016 0.595245 0.666006\n",
    "RidgeClassifier:\n",
    "0.806680 0.572035 0.585621 0.654779\n",
    "0.819700 0.583923 0.597792 0.667138\n",
    "over\n",
    "\n",
    "#不使用集合\n",
    "LogisticRegression:\n",
    "0.804416 0.568072 0.588169 0.653552\n",
    "0.817719 0.569488 0.596377 0.661194\n",
    "MultinomialNB:\n",
    "0.814888 0.562129 0.574299 0.650439\n",
    "BernoulliNB:\n",
    "0.827342 0.575998 0.584489 0.662610\n",
    "0.784602 0.518823 0.498160 0.600528\n",
    "SGDClassifier:\n",
    "0.799887 0.560713 0.581376 0.647325\n",
    "0.766770 0.505519 0.500991 0.591093\n",
    "LinearSVC:\n",
    "0.803849 0.559864 0.581376 0.648363\n",
    "RidgeClassifier:\n",
    "0.794509 0.547976 0.569771 0.637419\n",
    "0.816303 0.559015 0.585621 0.653647\n",
    "over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832080908526\n",
      "0.583931295553\n",
      "0.602519916379\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 've' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5840cc6a361a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mvu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mvu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"%f %f %f %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mvg\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mva\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mve\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mva\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3.0\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 've' is not defined"
     ]
    }
   ],
   "source": [
    "#VotingClassifier方案\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.naive_bayes import BernoulliNB  \n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "\n",
    "clf1 = LogisticRegression(C=2,penalty='l2')\n",
    "clf2 = MultinomialNB(alpha=0.01)\n",
    "clf3 = BernoulliNB(alpha=0.01)\n",
    "clf4 = SGDClassifier(alpha=5e-05)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('mb', clf2), ('bb', clf3) , ('sg', clf4)], voting='hard')\n",
    "eclf = eclf.fit(x_train, y_train_g)\n",
    "predictions = eclf.predict( x_test )\n",
    "vg = precision_score(y_test_g, predictions, average='micro')  \n",
    "print vg\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('mb', clf2), ('bb', clf3) , ('sg', clf4)], voting='hard')\n",
    "eclf = eclf.fit(x_train, y_train_a)\n",
    "predictions = eclf.predict( x_test )\n",
    "va = precision_score(y_test_a, predictions, average='micro')  \n",
    "print va\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('mb', clf2), ('bb', clf3) , ('sg', clf4)], voting='hard')\n",
    "eclf = eclf.fit(x_train, y_train_e)\n",
    "predictions = eclf.predict( x_test )\n",
    "vu = precision_score(y_test_e, predictions, average='micro')  \n",
    "print vu\n",
    "print( \"%f %f %f %f\" % ( vg , va , ve , (vg + va + ve) / 3.0 ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed: 15.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=4, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.75, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=2,\n",
      "       param_grid={'vect__max_features': (90000, 100000, 110000)},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)\n",
      "0.60606608507\n",
      " \t vect__max_features: 100000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression,RidgeClassifier,PassiveAggressiveClassifier,Lasso,HuberRegressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#    ('vect', TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)),\n",
    "#    ('clf', BernoulliNB(alpha=0.01)),\n",
    "    \n",
    "#    ('vect', TfidfVectorizer(max_df = 0.5 , ngram_range = (1, 2) , norm = 'l2')),\n",
    "#    ('clf', MultinomialNB(alpha = 0.01))\n",
    "    \n",
    "#    ('vect', TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)),\n",
    "#    ('clf', SGDClassifier())\n",
    "\n",
    "#    ('vect', TfidfVectorizer(max_features=10000,ngram_range=(1, 1), max_df=0.5, norm='l1', use_idf=True)),\n",
    "#    ('clf', PassiveAggressiveClassifier())\n",
    "    \n",
    "    ('vect', TfidfVectorizer(max_df = 0.75)),\n",
    "    ('clf', LogisticRegression(C=2,penalty='l2'))\n",
    "])\n",
    "parameters = {\n",
    "\n",
    "    #'vect__max_df': ( 0.5 , 0.75 , 1 ),\n",
    "    #'vect__stop_words': ('english', None ),\n",
    "    'vect__max_features': ( 90000 , 100000 , 110000 ),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    #'vect__use_idf': ( True , False ),\n",
    "    #'vect__norm': ('l1', 'l2'),\n",
    "    \n",
    "    #'clf__penalty': ('l1', 'l2'),\n",
    "    #'clf__C': ( 0.9 , 1 ),\n",
    "    #'clf__loss': ( 'squared_hinge' , 'hinge' ),\n",
    "    #'clf__n_iter': ( 20 , 30 )\n",
    "    #'clf__alpha': ( 0.001 , 0.01 , 0.5 )  \n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV( pipeline , parameters, n_jobs=2, verbose=1, scoring='accuracy', cv=4)\n",
    "grid.fit(final_cut_data, Agelist)\n",
    "#grid.fit(final_cut_data, Genderlist)\n",
    "#grid.fit(final_cut_data, Edulist)\n",
    "\n",
    "#Alllist\n",
    "#grid.fit(x_train, y_train)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "best_parameters = grid.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(' \\t %s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#predictions = grid_search.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agelist max_df: 0.75 max_features: 130000\n",
    "Edulist max_df: 0.75 max_features: 170000\n",
    "Genderlist max_df: 0.75 max_features: 90000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://prozhuchen.com/2016/12/28/CCF%E5%A4%A7%E8%B5%9B%E6%90%9C%E7%8B%97%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E6%80%BB%E7%BB%93/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
