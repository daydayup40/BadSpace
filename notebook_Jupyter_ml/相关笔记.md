# matplotlib
## 1. 画二位坐标图

```python
% matplotlib inline
import matplotlib.pyplot as plt

#创建一个初始化函数
def runplt():
  plt.figure()
  plt.title('标题')
  plt.xlabel('横坐标名称')
  plt.ylabel('纵坐标名称')
  plt.axis([0, 25, 0, 25]) #坐标显示范围
  plt.grid( True ) #是否打印网格:True打印
  return plt

plt = runplt()
x = [[6], [8], [10], [14], [18]]
y = [[7], [9], [13], [17.5], [18]]
plt.plot(x, y, 'k.')
plt.show()
```
其中plot方法：   
1，2参数为横纵坐标点向量组;   
3参数首字符表示颜色,b=蓝色、k=黑色、g=绿色...   
3参数第二字节表示画图方法:    
'.'=画点   
'-'=连线   
'-.'=虚线连线

# scikit-learn
## 1. 一元线性回归
```python
from sklearn.linear_model import LinearRegression
# 创建并拟合模型
model = LinearRegression()

x = [[6], [8], [10], [14], [18]]
y = [[7], [9], [13], [17.5], [18]]
model.fit(x, y)

ret = model.predict([[12],[3]])

print(' 预测12=%.2f ; 3=%.2f' %(ret[0] ,ret[1]))
```
```
预测12=13.57 ; 3=3.97
```

## 2. 模型评估R方
scikit-learn中的方法(score)计算的结果有可能是负值，说明拟合效果很糟糕，越接近1越好
```python
from sklearn.linear_model import LinearRegression
x = [[6], [8], [10], [14], [18]]
y = [[7], [9], [13], [17.5], [18]]
model = LinearRegression()
model.fit(x, y)
#测试数据
x_test = [[8], [9], [11], [16], [12]]
y_test = [[11], [8.5], [15], [18], [11]]

print( model.score(x_test, y_test))
#这里用学习数据进行测试只是个功能性实验
print( model.score(x, y))
```
```
0.662005292942
0.910001596424
```

# numpy
## 1. 关于array的笔记
###  (1) 直接元素对应操作
```python
import numpy as np
data = [1,2,3,4,5]
data2 = [1,1,1,2,2]
ary_d = np.array(data)
ary_d2 = np.array(data2)
print(ary_d)
print(ary_d + 1)
print(ary_d ** 2)
print(ary_d - ary_d2)
```
```
[1 2 3 4 5]
[2 3 4 5 6]
[ 1  4  9 16 25]
[0 1 2 2 3]
```

### (2) 直接方法调用
```python
import numpy as np

d = [1,2,3,4,5]
m = [5,0,0,0,1]

print('mean   : %.2f' % np.mean( d ))
print('average: %.2f' % np.dot( d , m ))

ary_d = np.array(d)
ary_m = np.array(m)

print('v_mean   : %.2f' % ary_d.mean())
print('v_average: %.2f' % ary_d.dot( ary_m ))
```
```
mean   : 3.00
average: 10.00
v_mean   : 3.00
v_average: 10.00
```

## 2. mean,average与dot
mean = 单纯计算算数平均值    
average = 可以计算加权平均值,权值为全1时一定与mean等效   
dot     = 计算內积
```python
import numpy as np

d = [1,2,3,4,5]
m = [5,0,0,0,1]

print('mean   : %.2f' % np.mean( d ))
print('average: %.2f' % np.average( d , weights=m ))

dot = np.dot(d , m)
mSum = np.sum(m)
print('by dot : %.2f' % (float(dot) / float(mSum)) )
```
```
#结果
mean   : 3.00
average: 1.67
by dot : 1.67
```
上述例子中：   
```
mean = (d1 + d2 + d3 + d4 + d5) / 5
average = (d1*m1 + d2*m2 + d3*m3 + d4*m4 + d5*m5) / (m1 + m2 + m3 + m4 + m5)
dot = d1*m1 + d2*m2 + d3*m3 + d4*m4 + d5*m5
```

## 3. 方差与协方差var、cov
### (1)方差var
```python
import numpy as np
d = [6, 8, 10, 14, 18]
aryd = np.array(d)
variance = ((aryd - aryd.mean())**2).sum() / (len(aryd) - 1)
print( 'ret = %.2f' % variance )
print( 'var = %.2f' % np.var(d, ddof=1))
print( 'var = %.2f' % aryd.var(ddof=1))
```
```
ret = 23.20
var = 23.20
var = 23.20
```
### (2)协方差cov
```python
import numpy as np
myX = np.array([6 , 8 , 10 , 14   , 18])
myY = np.array([7 , 9 , 13 , 17.5 , 18])
ret = (((myX - myX.mean()) * (myY - myY.mean())).sum() / (len(myX) - 1))
cov = np.cov(myX, myY)[0][1]
print( 'ret = %.2f' % ret )
print( 'cov = %.2f' % cov )
```
```
ret = 22.65
cov = 22.65
```

## 4. 矩阵运算相关
dot=內积或矩阵乘法    
inv=矩阵求逆    
transpose=矩阵求转置

## 5. 矩阵“除法运算”
### (1)直接的思路算法
对于 _Y = XB_ 这种类型，其中Y、B为列向量，X不一定是方阵，已知X、Y求B    
利用 _B = inv( T(X) * X ) * T(X) * Y_

```python
from numpy.linalg import inv
from numpy import dot, transpose
x = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]
y = [[7], [9], [13], [17.5], [18]]
print(dot(inv(dot(transpose(x), x)), dot(transpose(x), y)))
```
```
[[ 1.1875    ]
 [ 1.01041667]
 [ 0.39583333]]
```

### (2)Numpy提供的最小二乘法函数实现
```python
from numpy.linalg import lstsq
x = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]
y = [[7], [9], [13], [17.5], [18]]
print(lstsq(x, y)[0])
```
```
[[ 1.1875    ]
 [ 1.01041667]
 [ 0.39583333]]
```

.............
